# MoE (Mixture of Experts)

**Mixture of Experts** — это архитектурный подход, где модель состоит из **нескольких "экспертов" (специализированных подмоделей)**, но при этом **на каждом запросе активируется только часть из них**.

Идея: **"не всё знание нужно каждый раз"**, поэтому вместо того, чтобы использовать одну гигантскую монолитную модель, мы строим **набор экспертов**, каждый из которых умеет делать что-то лучше других — и даём модели **механизм выбора**, кого активировать.

### **Как это работает (механика)**

1. **Вход (текст, токены)** поступает в модель.
2. **"Router" (маршрутизатор)** — отдельный блок — принимает решение:
    — какие эксперты лучше справятся с этой задачей.
3. **Выбираются 2–4 эксперта из, скажем, 16**.
4. **Только выбранные эксперты получают вход и выдают свои выходы**.
5. **Выходы агрегируются** (обычно усредняются или складываются с весами) — и передаются дальше.

---

### **Сравнение с обычными моделями**

| Модель | Принцип работы | Плюсы | Минусы |
|----|----|----|----|
| **Monolithic** | Все параметры активны на каждом шаге | Простота, предсказуемость | Огромная вычислительная нагрузка |
| **MoE** | Только часть параметров активна на каждом шаге | Эффективность, масштабируемость | Сложность реализации и маршрутизации |

---

### **Зачем нужен MoE**

- **Масштабирование без роста затрат**: можешь создать модель на **1 триллион параметров**, но использовать всего **10%** на запрос.
- **Универсальность**: эксперты могут быть заточены под разные типы задач (перевод, код, математика).
- **Локализация ошибок**: легче дообучить или заменить отдельного эксперта, чем всю модель.

---

### **Ключевые компоненты MoE:**

1. **Эксперты** — подмодели (обычно MLP-блоки).
2. **Router** — принимает решение, кого активировать.
3. **Sparse Activation** — активация только N экспертов из всех.
4. **Load Balancing Loss** — регуляризатор, чтобы все эксперты использовались равномерно.

---

### **Архитектуры, использующие MoE:**

- **Switch Transformer (Google, 2021)** — упрощённый MoE: активен только **1 эксперт** на шаг.
- **GShard** (Google) — первый масштабируемый MoE на миллиарды токенов.
- **GLaM** (Google, 2021) — 1.2 триллиона параметров, но 97% из них никогда не активируются одновременно.
- **GPT-4 (возможно)** и **GPT-4-Turbo (почти точно)** — скрыто используют MoE.

---

### **Пример: простая MoE-схема**

```javascript
Input → [Router]
           ↓
       ┌──────────────┐
       │ Expert 1     │
       │ Expert 2     │  → (только они активны)
       │ Expert 3     │
       │ ...          │
       └──────────────┘
           ↓
     Aggregation
           ↓
        Output
```

---

### **Почему это важно**

Потому что MoE — **ключ к дальнейшему масштабированию моделей**. Обучать можно всё, использовать — выборочно. Это как иметь армию специалистов, но вызывать на дело только нужных.